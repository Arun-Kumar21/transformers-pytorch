{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:27:41.398376Z",
     "iopub.status.busy": "2025-08-22T04:27:41.397790Z",
     "iopub.status.idle": "2025-08-22T04:27:47.877348Z",
     "shell.execute_reply": "2025-08-22T04:27:47.876522Z",
     "shell.execute_reply.started": "2025-08-22T04:27:41.398355Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:27:50.208101Z",
     "iopub.status.busy": "2025-08-22T04:27:50.207663Z",
     "iopub.status.idle": "2025-08-22T04:27:50.217620Z",
     "shell.execute_reply": "2025-08-22T04:27:50.216951Z",
     "shell.execute_reply.started": "2025-08-22T04:27:50.208079Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['valid.en', 'valid.de', 'de_en.csv', 'train.de', 'train.en']\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/kaggle/input/en-de-dataset/'\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:27:53.197817Z",
     "iopub.status.busy": "2025-08-22T04:27:53.197570Z",
     "iopub.status.idle": "2025-08-22T04:27:58.767607Z",
     "shell.execute_reply": "2025-08-22T04:27:58.766746Z",
     "shell.execute_reply.started": "2025-08-22T04:27:53.197799Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633faf165a944228929c7c82a522e0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf1447a909e44f3ab398312a85e4f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441e44bc9c07404fb2f4776e472d451d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2af8233ff24afebf599e268119a7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da94c5018fc4c49a218411a3b2ac127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:28:14.138742Z",
     "iopub.status.busy": "2025-08-22T04:28:14.137910Z",
     "iopub.status.idle": "2025-08-22T04:28:14.143748Z",
     "shell.execute_reply": "2025-08-22T04:28:14.142749Z",
     "shell.execute_reply.started": "2025-08-22T04:28:14.138711Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16816, 2, 406, 85, 787, 548, 41, 31, 0] [36, 11689, 19, 1442, 2, 4813, 22, 46, 56, 31, 0]\n"
     ]
    }
   ],
   "source": [
    "enc1 = tokenizer.encode(\"Hello, how can i help you?\")\n",
    "enc2 = tokenizer.encode(\"The moon is beautiful, isn't it?\")\n",
    "\n",
    "print(enc1, enc2)\n",
    "# Some encoding chars ==> (, -> 2) , (? -> 31), (<s> -> 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:28:20.918062Z",
     "iopub.status.busy": "2025-08-22T04:28:20.917777Z",
     "iopub.status.idle": "2025-08-22T04:28:21.396163Z",
     "shell.execute_reply": "2025-08-22T04:28:21.395286Z",
     "shell.execute_reply.started": "2025-08-22T04:28:20.918041Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ENGLISH</th>\n",
       "      <th>GERMAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hi</td>\n",
       "      <td>hallo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hi</td>\n",
       "      <td>gru gott</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>run</td>\n",
       "      <td>lauf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>wow</td>\n",
       "      <td>potzdonner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>wow</td>\n",
       "      <td>donnerwetter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 ENGLISH        GERMAN\n",
       "0           0      hi         hallo\n",
       "1           1      hi      gru gott\n",
       "2           2     run          lauf\n",
       "3           3     wow    potzdonner\n",
       "4           4     wow  donnerwetter"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_dir + '/de_en.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:28:24.928280Z",
     "iopub.status.busy": "2025-08-22T04:28:24.927765Z",
     "iopub.status.idle": "2025-08-22T04:28:24.946888Z",
     "shell.execute_reply": "2025-08-22T04:28:24.946102Z",
     "shell.execute_reply.started": "2025-08-22T04:28:24.928257Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 152820\n"
     ]
    }
   ],
   "source": [
    "en_sentences = df['ENGLISH'].astype(str).tolist()\n",
    "de_sentences = df['GERMAN'].astype(str).tolist()\n",
    "\n",
    "en_sentences = en_sentences\n",
    "de_sentences = de_sentences\n",
    "\n",
    "\n",
    "print(f\"Total pairs: {len(en_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:28:28.361102Z",
     "iopub.status.busy": "2025-08-22T04:28:28.360397Z",
     "iopub.status.idle": "2025-08-22T04:28:28.380733Z",
     "shell.execute_reply": "2025-08-22T04:28:28.379987Z",
     "shell.execute_reply.started": "2025-08-22T04:28:28.361080Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 9000\n",
      "Validation size: 1000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "group = list(zip(en_sentences[:10000], de_sentences[:10000]))\n",
    "random.shuffle(group)\n",
    "\n",
    "# 90% train, 10% validation\n",
    "train_group = group[:int(len(group) * 0.9)]\n",
    "val_group = group[int(len(group) * 0.9):]\n",
    "\n",
    "print(f\"Train size: {len(train_group)}\")\n",
    "print(f\"Validation size: {len(val_group)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:29:05.798528Z",
     "iopub.status.busy": "2025-08-22T04:29:05.797853Z",
     "iopub.status.idle": "2025-08-22T04:29:05.806117Z",
     "shell.execute_reply": "2025-08-22T04:29:05.805216Z",
     "shell.execute_reply.started": "2025-08-22T04:29:05.798493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EnDePairDataset(Dataset):\n",
    "    def __init__(self, en_sentences, de_sentences, tokenizer):\n",
    "        self.en_sentences = en_sentences\n",
    "        self.de_sentences = de_sentences\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.en_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"return (En tensor, De tensor)\"\n",
    "        self.en_sentence = self.en_sentences[idx]\n",
    "        self.de_sentence = self.de_sentences[idx]\n",
    "\n",
    "        en_encoded = self.tokenizer.encode(self.en_sentence)\n",
    "        de_encoded = self.tokenizer.encode(self.de_sentence)\n",
    "        return torch.tensor(en_encoded, dtype=torch.long), torch.tensor(de_encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:29:08.978510Z",
     "iopub.status.busy": "2025-08-22T04:29:08.977590Z",
     "iopub.status.idle": "2025-08-22T04:29:09.031525Z",
     "shell.execute_reply": "2025-08-22T04:29:09.030736Z",
     "shell.execute_reply.started": "2025-08-22T04:29:08.978474Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_dataset, val_dataset:\n",
      "9000 1000\n",
      "First example:\n",
      "(tensor([16478,     0]), tensor([7475,  166,    0]))\n",
      "hi</s>\n",
      "hallo</s>\n"
     ]
    }
   ],
   "source": [
    "train_ds = EnDePairDataset(en_sentences[:9000], de_sentences[:9000], tokenizer)\n",
    "val_ds = EnDePairDataset(en_sentences[:1000], de_sentences[:1000], tokenizer)\n",
    "\n",
    "print(\"Length of train_dataset, val_dataset:\")\n",
    "print(len(train_ds), len(val_ds))\n",
    "\n",
    "print(\"First example:\")\n",
    "\n",
    "print(train_ds[0])\n",
    "print(tokenizer.decode(train_ds[0][0]))\n",
    "print(tokenizer.decode(train_ds[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:29:23.768897Z",
     "iopub.status.busy": "2025-08-22T04:29:23.768074Z",
     "iopub.status.idle": "2025-08-22T04:29:23.778036Z",
     "shell.execute_reply": "2025-08-22T04:29:23.777284Z",
     "shell.execute_reply.started": "2025-08-22T04:29:23.768863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "max_seq_len = 100\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"Return fixed size tensor\"\n",
    "    srcs, tgts = zip(*batch)\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    src_batch = []\n",
    "    tgt_batch = []\n",
    "\n",
    "    for src in srcs:\n",
    "        if len(src) < max_seq_len:\n",
    "            \"Padding\"\n",
    "            padded = torch.cat([src, torch.tensor([pad_idx] * (max_seq_len - len(src)), dtype=torch.long)])\n",
    "        else:\n",
    "            \"Truncate\"\n",
    "            padded = src[:max_seq_len]\n",
    "        src_batch.append(padded)\n",
    "\n",
    "    for tgt in tgts:\n",
    "        if len(tgt) < max_seq_len:\n",
    "            padded = torch.cat([tgt, torch.tensor([pad_idx] * (max_seq_len - len(tgt)), dtype=torch.long)])\n",
    "        else:\n",
    "            padded = tgt[:max_seq_len]\n",
    "        tgt_batch.append(padded)\n",
    "\n",
    "    src_batch = torch.stack(src_batch)\n",
    "    tgt_batch = torch.stack(tgt_batch)\n",
    "\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:29:28.967535Z",
     "iopub.status.busy": "2025-08-22T04:29:28.967220Z",
     "iopub.status.idle": "2025-08-22T04:29:28.973970Z",
     "shell.execute_reply": "2025-08-22T04:29:28.973171Z",
     "shell.execute_reply.started": "2025-08-22T04:29:28.967512Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# (batch_size, seq)-> (64, 2150)\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:29:38.807329Z",
     "iopub.status.busy": "2025-08-22T04:29:38.806682Z",
     "iopub.status.idle": "2025-08-22T04:29:38.876339Z",
     "shell.execute_reply": "2025-08-22T04:29:38.875504Z",
     "shell.execute_reply.started": "2025-08-22T04:29:38.807304Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "lr = 3e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_embd = 512\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.1\n",
    "vocab_size = tokenizer.vocab_size\n",
    "max_seq_len=100\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "\n",
    "import copy\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:30:16.157962Z",
     "iopub.status.busy": "2025-08-22T04:30:16.157218Z",
     "iopub.status.idle": "2025-08-22T04:30:16.162328Z",
     "shell.execute_reply": "2025-08-22T04:30:16.161619Z",
     "shell.execute_reply.started": "2025-08-22T04:30:16.157926Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "  def __init__(self, vocab_size, n_embd):\n",
    "    super(Embeddings, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "    self.scale = n_embd ** 0.5\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.embedding(x) * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:30:20.127327Z",
     "iopub.status.busy": "2025-08-22T04:30:20.126794Z",
     "iopub.status.idle": "2025-08-22T04:30:20.178351Z",
     "shell.execute_reply": "2025-08-22T04:30:20.177716Z",
     "shell.execute_reply.started": "2025-08-22T04:30:20.127308Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for idx, (src, tgt) in enumerate(train_loader):\n",
    "  x = src\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:30:23.777587Z",
     "iopub.status.busy": "2025-08-22T04:30:23.777280Z",
     "iopub.status.idle": "2025-08-22T04:30:24.102150Z",
     "shell.execute_reply": "2025-08-22T04:30:24.101472Z",
     "shell.execute_reply.started": "2025-08-22T04:30:23.777566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n",
      "torch.Size([64, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape) # (B, S)\n",
    "\n",
    "y = Embeddings(vocab_size, n_embd)(x)\n",
    "print(y.shape) # (B, S, n_embd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:31:04.442155Z",
     "iopub.status.busy": "2025-08-22T04:31:04.441868Z",
     "iopub.status.idle": "2025-08-22T04:31:04.447717Z",
     "shell.execute_reply": "2025-08-22T04:31:04.446968Z",
     "shell.execute_reply.started": "2025-08-22T04:31:04.442131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, n_embd, max_len=100):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    pe = torch.zeros(max_len, n_embd)\n",
    "    pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, n_embd, 2) * -(math.log(10000.0) / n_embd)) # e ^ [-ln(10000) * (2i/ n_embd)]\n",
    "    pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "    pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "    pe = pe.unsqueeze(0)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:31:13.207068Z",
     "iopub.status.busy": "2025-08-22T04:31:13.206788Z",
     "iopub.status.idle": "2025-08-22T04:31:13.264772Z",
     "shell.execute_reply": "2025-08-22T04:31:13.264086Z",
     "shell.execute_reply.started": "2025-08-22T04:31:13.207049Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "# shape of positionalEncoding and embeddings will be same..\n",
    "\n",
    "pos_y = PositionalEncoding(n_embd)(y)\n",
    "print(pos_y.shape) # (B, S, n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:31:16.547021Z",
     "iopub.status.busy": "2025-08-22T04:31:16.546735Z",
     "iopub.status.idle": "2025-08-22T04:31:16.551072Z",
     "shell.execute_reply": "2025-08-22T04:31:16.550248Z",
     "shell.execute_reply.started": "2025-08-22T04:31:16.547000Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "  \"Create N identical layers.\"\n",
    "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:31:20.987070Z",
     "iopub.status.busy": "2025-08-22T04:31:20.986336Z",
     "iopub.status.idle": "2025-08-22T04:31:20.991689Z",
     "shell.execute_reply": "2025-08-22T04:31:20.990906Z",
     "shell.execute_reply.started": "2025-08-22T04:31:20.987046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def attention(q, k, v, mask=None, dropout=None):\n",
    "  \"Compute Scaled Dot Product Attention (Attention score)\"\n",
    "  dim_k = k.size(-1)\n",
    "\n",
    "  # Q (B, S, dim_k) , K (B, S, dim_k)\n",
    "  # For Q @ K, we need to transpose K -> K (B, dim_k, S) ---> more precisely Q(S, D) * K (D, S) => (S, S)\n",
    "  score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(dim_k) # score = (Q* K) / sqrt(d_k)  --> single_layer (d_k) = n_embd || multi_layer(d_k) = n_embd / n_head\n",
    "  if mask is not None:\n",
    "    score = score.masked_fill(mask == 0, -1e9)\n",
    "  att_w = score.softmax(dim=-1)\n",
    "  if dropout is not None:\n",
    "    att_w = dropout(att_w)\n",
    "\n",
    "\n",
    "  # att_w (B, S, S) , V (B, S, d_k or n_embd [in case of single attention head])\n",
    "  # att_w @ V ---> (B, S, d_k or n_embd)\n",
    "  return torch.matmul(att_w, v), att_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:31:24.016815Z",
     "iopub.status.busy": "2025-08-22T04:31:24.016534Z",
     "iopub.status.idle": "2025-08-22T04:31:24.155215Z",
     "shell.execute_reply": "2025-08-22T04:31:24.154493Z",
     "shell.execute_reply.started": "2025-08-22T04:31:24.016794Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100, 512])\n",
      "torch.Size([64, 100, 512])\n",
      "torch.Size([64, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "# Single layer attention\n",
    "\n",
    "q = nn.Linear(n_embd, n_embd)(pos_y)\n",
    "k = nn.Linear(n_embd, n_embd)(pos_y)\n",
    "v = nn.Linear(n_embd, n_embd)(pos_y)\n",
    "\n",
    "print(q.shape) # (B, S, n_embd)\n",
    "\n",
    "attn_y, attn_w = attention(q, k, v)\n",
    "print(attn_y.shape) # (B, S, n_embd)\n",
    "print(attn_w.shape) # (B, S, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:31:29.592955Z",
     "iopub.status.busy": "2025-08-22T04:31:29.592680Z",
     "iopub.status.idle": "2025-08-22T04:31:29.599643Z",
     "shell.execute_reply": "2025-08-22T04:31:29.598947Z",
     "shell.execute_reply.started": "2025-08-22T04:31:29.592933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "  def __init__(self, n_head, n_embd, dropout=0.1):\n",
    "    super(MultiHeadedAttention, self).__init__()\n",
    "    assert n_embd % n_head == 0, \"can't divide n_embd by n_head\"\n",
    "    self.n_head = n_head\n",
    "    self.n_embd = n_embd\n",
    "    self.dim_k = n_embd // n_head # d_k\n",
    "    self.Ws = clones(nn.Linear(n_embd, n_embd), 4)\n",
    "    self.attn = None\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, q, k, v, mask=None):\n",
    "    if mask is not None:\n",
    "      mask = mask.unsqueeze(1)\n",
    "    n_batches = q.size(0)\n",
    "\n",
    "      \n",
    "    # projecting q, k, v (passing through FC-linear layer)\n",
    "    Q = self.Ws[0](q).view(n_batches, -1, self.n_head, self.dim_k).transpose(1, 2) # Q = q @ W_q \n",
    "    K = self.Ws[1](k).view(n_batches, -1, self.n_head, self.dim_k).transpose(1, 2) # K = k @ W_k\n",
    "    V = self.Ws[2](v).view(n_batches, -1, self.n_head, self.dim_k).transpose(1, 2) # V = v @ W_v\n",
    "\n",
    "    # W_q, W_k, W_v are learnable weight matrices (from linear layer)\n",
    "\n",
    "    x, self.attn = attention(Q, K, V, mask=mask, dropout=self.dropout)\n",
    "\n",
    "    \"Concatenating all heads\"\n",
    "    x = x.transpose(1, 2).contiguous().view(n_batches, -1, self.n_head * self.dim_k)\n",
    "\n",
    "    return self.Ws[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:31:37.186507Z",
     "iopub.status.busy": "2025-08-22T04:31:37.186229Z",
     "iopub.status.idle": "2025-08-22T04:31:37.191554Z",
     "shell.execute_reply": "2025-08-22T04:31:37.190806Z",
     "shell.execute_reply.started": "2025-08-22T04:31:37.186482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "  \"Normalize features\"\n",
    "  def __init__(self, size, eps=1e-6):\n",
    "    super(LayerNorm, self).__init__()\n",
    "    self.a_2 = nn.Parameter(torch.ones(size))\n",
    "    self.b_2 = nn.Parameter(torch.zeros(size))\n",
    "    self.eps = eps\n",
    "\n",
    "  def forward(self, x):\n",
    "    mean = x.mean(1, keepdim=True)\n",
    "    std = x.std(1, keepdim=True)\n",
    "    return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:31:39.897204Z",
     "iopub.status.busy": "2025-08-22T04:31:39.896621Z",
     "iopub.status.idle": "2025-08-22T04:31:39.901539Z",
     "shell.execute_reply": "2025-08-22T04:31:39.900730Z",
     "shell.execute_reply.started": "2025-08-22T04:31:39.897177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "  \"Residual connection followed by layer norm\"\n",
    "  def __init__(self, size, dropout):\n",
    "    super(SublayerConnection, self).__init__()\n",
    "    self.layer_norm = LayerNorm(size)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, sublayer):\n",
    "    return x + self.dropout(sublayer(self.layer_norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:31:42.107057Z",
     "iopub.status.busy": "2025-08-22T04:31:42.106788Z",
     "iopub.status.idle": "2025-08-22T04:31:42.111583Z",
     "shell.execute_reply": "2025-08-22T04:31:42.110940Z",
     "shell.execute_reply.started": "2025-08-22T04:31:42.107037Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "  def __init__(self, n_embd, dropout):\n",
    "    super(FeedForwardLayer, self).__init__()\n",
    "    self.ff = nn.Sequential(\n",
    "        nn.Linear(n_embd, 4 * n_embd),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4 * n_embd, n_embd),\n",
    "        nn.Dropout(dropout)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:31:45.596722Z",
     "iopub.status.busy": "2025-08-22T04:31:45.596478Z",
     "iopub.status.idle": "2025-08-22T04:31:45.601902Z",
     "shell.execute_reply": "2025-08-22T04:31:45.601159Z",
     "shell.execute_reply.started": "2025-08-22T04:31:45.596706Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "  \"Consist of multi-head attention and feed forward\"\n",
    "  def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "    self.self_attn = self_attn\n",
    "    self.feed_forward = feed_forward\n",
    "    self.add_and_norm = clones(SublayerConnection(size, dropout), 2)\n",
    "    self.size = size\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    x = self.add_and_norm[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "    x = self.add_and_norm[1](x, self.feed_forward)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:31:48.811941Z",
     "iopub.status.busy": "2025-08-22T04:31:48.811424Z",
     "iopub.status.idle": "2025-08-22T04:31:48.817313Z",
     "shell.execute_reply": "2025-08-22T04:31:48.816476Z",
     "shell.execute_reply.started": "2025-08-22T04:31:48.811917Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "  \"Consist of multi-head attn, src_attn, feed forward\"\n",
    "  def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "    self.size = size\n",
    "    self.self_attn = self_attn\n",
    "    self.src_attn = src_attn\n",
    "    self.feed_forward = feed_forward\n",
    "    self.add_and_norm = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "  def forward(self, x, encoder_op, src_mask, tgt_mask):\n",
    "    x = self.add_and_norm[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "    x = self.add_and_norm[1](x, lambda x: self.src_attn(x, encoder_op, encoder_op, src_mask))\n",
    "    x = self.add_and_norm[2](x, self.feed_forward)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:31:50.406480Z",
     "iopub.status.busy": "2025-08-22T04:31:50.406190Z",
     "iopub.status.idle": "2025-08-22T04:31:50.411003Z",
     "shell.execute_reply": "2025-08-22T04:31:50.410354Z",
     "shell.execute_reply.started": "2025-08-22T04:31:50.406460Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, layer, N):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.layers = clones(layer, N)\n",
    "    self.norm = LayerNorm(layer.size)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, mask)\n",
    "    return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:31:50.596728Z",
     "iopub.status.busy": "2025-08-22T04:31:50.596468Z",
     "iopub.status.idle": "2025-08-22T04:31:50.601060Z",
     "shell.execute_reply": "2025-08-22T04:31:50.600502Z",
     "shell.execute_reply.started": "2025-08-22T04:31:50.596709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, layer, N):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.layers = clones(layer, N)\n",
    "    self.norm = LayerNorm(layer.size)\n",
    "\n",
    "  def forward(self, x, encoder_op, src_mask, tgt_mask):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, encoder_op, src_mask, tgt_mask)\n",
    "    return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:34:12.023906Z",
     "iopub.status.busy": "2025-08-22T04:34:12.023642Z",
     "iopub.status.idle": "2025-08-22T04:34:12.030778Z",
     "shell.execute_reply": "2025-08-22T04:34:12.029976Z",
     "shell.execute_reply.started": "2025-08-22T04:34:12.023885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transfomer(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Transfomer, self).__init__()\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(n_head, n_embd, dropout)\n",
    "    ff = FeedForwardLayer(n_embd, dropout)\n",
    "    position = PositionalEncoding(n_embd, max_seq_len)\n",
    "\n",
    "    self.encoder = Encoder(EncoderLayer(n_embd, c(attn), c(ff), dropout), n_layer)\n",
    "    self.decoder = Decoder(DecoderLayer(n_embd, c(attn), c(attn), c(ff), dropout), n_layer)\n",
    "    self.src_embed = nn.Sequential(Embeddings(vocab_size, n_embd), c(position))\n",
    "    self.tgt_embed = nn.Sequential(Embeddings(vocab_size, n_embd), c(position))\n",
    "    self.generator = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "  def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "    \"Process src and tgt sequences.\"\n",
    "    encoded_src = self.encode(src, src_mask)\n",
    "    decoded_tgt = self.decode(encoded_src, src_mask, tgt, tgt_mask)\n",
    "\n",
    "    return self.generator(decoded_tgt)\n",
    "\n",
    "  def encode(self, src, src_mask):\n",
    "    return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "  def decode(self, encoder_op, src_mask, tgt, tgt_mask):\n",
    "    tgt_mask = make_tgt_mask(tgt, pad_idx).to(tgt.device)\n",
    "    return self.decoder(self.tgt_embed(tgt), encoder_op, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:34:12.218999Z",
     "iopub.status.busy": "2025-08-22T04:34:12.218393Z",
     "iopub.status.idle": "2025-08-22T04:34:12.223727Z",
     "shell.execute_reply": "2025-08-22T04:34:12.223013Z",
     "shell.execute_reply.started": "2025-08-22T04:34:12.218970Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_src_mask(src, pad_idx):\n",
    "    \"Create a mask to hide padding tokens in the source sequence.\"\n",
    "    return (src != pad_idx).unsqueeze(1)\n",
    "\n",
    "def make_tgt_mask(tgt, pad_idx):\n",
    "    \"Create a mask to hide padding tokens in the target sequence and future tokens.\"\n",
    "    # (B, seq_len, seq_len)\n",
    "    tgt_pad_mask = (tgt != pad_idx).unsqueeze(-2)\n",
    "    tgt_seq_len = tgt.size(-1)\n",
    "    subsequent_mask = torch.tril(torch.ones(tgt_seq_len, tgt_seq_len, dtype=torch.bool, device=tgt.device))\n",
    "    # (B, seq_len, seq_len) & (1, seq_len, seq_len) -> (B, seq_len, seq_len)\n",
    "    return tgt_pad_mask & subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:34:15.299320Z",
     "iopub.status.busy": "2025-08-22T04:34:15.298680Z",
     "iopub.status.idle": "2025-08-22T04:34:16.524358Z",
     "shell.execute_reply": "2025-08-22T04:34:16.523745Z",
     "shell.execute_reply.started": "2025-08-22T04:34:15.299296Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 118.728949 M\n"
     ]
    }
   ],
   "source": [
    "model = Transfomer().to(device)\n",
    "\n",
    "p = sum(p.nelement() for p in model.parameters())\n",
    "print(f\"Number of parameters: {p / 1e6} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T04:35:31.535987Z",
     "iopub.status.busy": "2025-08-22T04:35:31.535352Z",
     "iopub.status.idle": "2025-08-22T04:38:57.567976Z",
     "shell.execute_reply": "2025-08-22T04:38:57.567153Z",
     "shell.execute_reply.started": "2025-08-22T04:35:31.535964Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 / 141, training loss: 12.52568531036377\n",
      "Batch 100 / 141, training loss: 3.4074044227600098\n",
      "Epoch 1, training loss: 4.0851633514918335\n",
      "Model saved after epoch 1 to /kaggle/working/transfomer_En-de.pth\n",
      "Epoch 1, validation loss: 1.8461016342043877\n",
      "Epoch 1, time: 66.75939011573792\n",
      "Batch 0 / 141, training loss: 2.3023734092712402\n",
      "Batch 100 / 141, training loss: 1.411494493484497\n",
      "Epoch 2, training loss: 1.9508716028632846\n",
      "Model saved after epoch 2 to /kaggle/working/transfomer_En-de.pth\n",
      "Epoch 2, validation loss: 0.8390398770570755\n",
      "Epoch 2, time: 66.4373984336853\n",
      "Batch 0 / 141, training loss: 1.2990890741348267\n",
      "Batch 100 / 141, training loss: 1.1280661821365356\n",
      "Epoch 3, training loss: 1.1933070657946538\n",
      "Model saved after epoch 3 to /kaggle/working/transfomer_En-de.pth\n",
      "Epoch 3, validation loss: 0.40727472491562366\n",
      "Epoch 3, time: 66.69695401191711\n",
      "Batch 0 / 141, training loss: 0.7663459181785583\n",
      "Training interrupted. Saving model...\n",
      "Model saved to /kaggle/working/transfomer_En-de.pth\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# lr decay\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "epochs = 50\n",
    "model_save_path = '/kaggle/working/' + 'transfomer_En-de.pth'\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "try:\n",
    "  for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for i, (src, tgt) in enumerate(train_loader):\n",
    "      src = src.to(device)\n",
    "      tgt = tgt.to(device)\n",
    "\n",
    "      src_mask = make_src_mask(src, pad_idx).to(device)\n",
    "      tgt_input = tgt[:, :-1]\n",
    "      tgt_output = tgt[:, 1:]\n",
    "      tgt_mask = make_tgt_mask(tgt_input, pad_idx).to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "      loss = criterion(output.contiguous().view(-1, output.size(-1)), tgt_output.contiguous().view(-1))\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      total_train_loss += loss.item()\n",
    "\n",
    "      if i % 100 == 0:\n",
    "        print(f\"Batch {i} / {len(train_loader)}, training loss: {loss.item()}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch+1}, training loss: {avg_train_loss}\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved after epoch {epoch+1} to {model_save_path}\")\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for i, (src, tgt) in enumerate(val_loader):\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        src_mask = make_src_mask(src, pad_idx).to(device)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        tgt_mask = make_tgt_mask(tgt_input, pad_idx).to(device)\n",
    "\n",
    "        output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "        loss = criterion(output.contiguous().view(-1, output.size(-1)), tgt_output.contiguous().view(-1))\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch+1}, validation loss: {avg_val_loss}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Epoch {epoch+1}, time: {end_time - start_time}\")\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Saving model...\")\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model_load_path = model_save_path\n",
    "\n",
    "loaded_model = Transfomer().to(device)\n",
    "loaded_model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T11:55:13.111023Z",
     "iopub.status.busy": "2025-07-20T11:55:13.110276Z",
     "iopub.status.idle": "2025-07-20T11:55:13.115752Z",
     "shell.execute_reply": "2025-07-20T11:55:13.114780Z",
     "shell.execute_reply.started": "2025-07-20T11:55:13.110999Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n"
     ]
    }
   ],
   "source": [
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "\n",
    "print(tokenizer.bos_token_id, tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T12:00:40.795353Z",
     "iopub.status.busy": "2025-07-20T12:00:40.795083Z",
     "iopub.status.idle": "2025-07-20T12:00:40.802010Z",
     "shell.execute_reply": "2025-07-20T12:00:40.801303Z",
     "shell.execute_reply.started": "2025-07-20T12:00:40.795333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, tokenizer, model, device, max_len=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode source sentence\n",
    "        src_tokens = tokenizer.encode(sentence, return_tensors='pt').to(device)\n",
    "        src_mask = (src_tokens != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Encode the source\n",
    "        memory = model.encode(src_tokens, src_mask)\n",
    "\n",
    "        # Init target with BOS\n",
    "        if tokenizer.bos_token_id is not None:\n",
    "            tgt_indices = [tokenizer.bos_token_id]\n",
    "        elif tokenizer.cls_token_id is not None:\n",
    "            tgt_indices = [tokenizer.cls_token_id]\n",
    "        else:\n",
    "            # fallback to first token of tokenizer\n",
    "            tgt_indices = [tokenizer.convert_tokens_to_ids(tokenizer.cls_token)]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "            tgt_mask = (tgt_tensor != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "            output = model.decode(memory, src_mask, tgt_tensor, tgt_mask)\n",
    "            logits = model.generator(output[:, -1])\n",
    "            next_token = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "            # Stop if EOS is generated\n",
    "            if next_token == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            tgt_indices.append(next_token)\n",
    "\n",
    "        # Decode output tokens\n",
    "        return tokenizer.decode(tgt_indices, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "english_sentence = \"I love you Khushi\"\n",
    "german_translation = translate_sentence(english_sentence, tokenizer, loaded_model, device)\n",
    "print(\"German Translation:\", german_translation)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7905149,
     "sourceId": 12523358,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
