# Transformers

Trying to implement various transformers; currently working on the En-De Transformer from the original "Attention Is All You Need" paper.

## References

- [Attention Is All You Need (Original Paper)](https://arxiv.org/abs/1706.03762)
- [Annotated Transformer (Harvard NLP)](https://nlp.seas.harvard.edu/2018/04/03/attention.html)